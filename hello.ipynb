{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T15:55:20.488627Z",
     "start_time": "2024-07-04T15:55:17.085915Z"
    }
   },
   "source": [
    "import torch\n",
    "import pretrainedmodels"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:01:35.049469Z",
     "start_time": "2024-07-04T16:01:34.552535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dict = torch.load('/home/caleb/Downloads/resnet34.fold0.best.pt', map_location=torch.device('cpu'))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n"
   ],
   "id": "94a8613ad94bd988",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:01:35.779459Z",
     "start_time": "2024-07-04T16:01:35.754564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, base_model, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer0 = nn.Sequential(*list(base_model.children())[:4])\n",
    "        self.layer1 = nn.Sequential(*list(base_model.layer1))\n",
    "        self.layer2 = nn.Sequential(*list(base_model.layer2))\n",
    "        self.layer3 = nn.Sequential(*list(base_model.layer3))\n",
    "        self.layer4 = nn.Sequential(*list(base_model.layer4))\n",
    "        self.dense1 = nn.Sequential(nn.Linear(n_features, 128))\n",
    "        self.dense2 = nn.Sequential(nn.Linear(128, 64))\n",
    "        self.classif = nn.Sequential(nn.Linear(64, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.classif(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x "
   ],
   "id": "f79da61558637070",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T16:01:38.056360Z",
     "start_time": "2024-07-04T16:01:36.549398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_model = pretrainedmodels.resnet34(num_classes=1000,pretrained='imagenet').to(device) #load pretrained as base\n",
    "model = Net(base_model, 512).to(device) # create model\n",
    "model.load_state_dict(model_dict) #loading weights\n",
    "model.eval()"
   ],
   "id": "9e22a10e188a7d04",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (layer0): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (dense1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "  )\n",
       "  (dense2): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (classif): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "classes = [\n",
    "    \"Sick\",\n",
    "    \"Not Sick\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pass\n",
    "    "
   ],
   "id": "1a7fea6b642083d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils import data\n",
    "import pretrainedmodels\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "from torch.utils.data.sampler import WeightedRandomSampler, BatchSampler\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, Normalize, RandomGamma, RandomBrightnessContrast, HueSaturationValue, CLAHE, ChannelShuffle, \n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose, PadIfNeeded, RandomCrop, Resize\n",
    ")\n",
    "import pretrainedmodels.utils as utils\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch, log_interval, loss_f, samples_per_epoch, device, cycling_optimizer=False):\n",
    "    \"\"\"Trains the model using the provided optimizer and loss function.\n",
    "    Shows output each log_interval iterations \n",
    "    Args:\n",
    "        model: Pytorch model to train.\n",
    "        train_loader: Data loader.\n",
    "        optimizer: pytroch optimizer.\n",
    "        epoch: Current epoch.\n",
    "        log_interval: Show model training progress each log_interval steps.\n",
    "        loss_f: Loss function to optimize.\n",
    "        samples_per_epoch: Number of samples per epoch to scale loss.\n",
    "        device: pytorch device\n",
    "        cycling_optimizer: Indicates of optimizer is cycling.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_losses = []\n",
    "    losses =[]\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x.to(device, dtype=torch.float))\n",
    "        loss = loss_f(output, target.to(device, dtype=torch.float))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), 4)\n",
    "        if cycling_optimizer:\n",
    "            optimizer.batch_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.3f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(x), samples_per_epoch,\n",
    "                100. * batch_idx * len(x) / samples_per_epoch, np.mean(losses)))\n",
    "            total_losses.append(np.mean(losses))\n",
    "            losses = []\n",
    "    train_loss_mean = np.mean(total_losses)\n",
    "    print('Mean train loss on epoch {} : {}'.format(epoch, train_loss_mean))\n",
    "    return train_loss_mean\n",
    "            \n",
    "def test(model, test_loader, loss_f, epoch, device):\n",
    "    \"\"\"Test the model with validation data.\n",
    "    Args:\n",
    "        model: Pytorch model to test data with.\n",
    "        test_loader: Data loader.\n",
    "        loss_f: Loss function.\n",
    "        epoch: Current epoch.\n",
    "        device: pytorch device        \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    predictions=[]\n",
    "    targets=[]\n",
    "    test_loss=[]\n",
    "    with torch.no_grad():\n",
    "        for x, target in test_loader:\n",
    "            output = model(x.to(device, dtype=torch.float))\n",
    "            test_loss.append(loss_f(output, target.to(device, dtype=torch.float)).item())\n",
    "            predictions.append(output.cpu())\n",
    "            targets.append(target.cpu())\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    score = roc_auc_score(targets, predictions)\n",
    "    test_loss  = np.mean(test_loss)\n",
    "    print('\\nTest set: Average loss: {:.6f}, roc auc: {:.4f}\\n'.format(test_loss, score))\n",
    "    return test_loss, score\t\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Build the nn network based on pretrained resnet models.\n",
    "    Args:\n",
    "        base_model: resnet34\\resnet50\\etc from pretrained models\n",
    "        n_features: n features from last pooling layer       \n",
    "    \"\"\"\n",
    "    def __init__(self, base_model, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer0 = nn.Sequential(*list(base_model.children())[:4])\n",
    "        self.layer1 = nn.Sequential(*list(base_model.layer1))\n",
    "        self.layer2 = nn.Sequential(*list(base_model.layer2))\n",
    "        self.layer3 = nn.Sequential(*list(base_model.layer3))\n",
    "        self.layer4 = nn.Sequential(*list(base_model.layer4))\n",
    "        self.dense1 = nn.Sequential(nn.Linear(n_features, 128))\n",
    "        self.dense2 = nn.Sequential(nn.Linear(128, 64))\n",
    "        self.classif = nn.Sequential(nn.Linear(64, 1))\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.classif(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    def features(self, x):\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "\n",
    "class CyclicLR(object):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `batch_step` should be called after a batch has been used for training.\n",
    "    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for eachparam groups.\n",
    "            Default: 0.001\n",
    "        max_lr (float or list): Upper boundaries in the cycle for\n",
    "            each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function. Default: 0.006\n",
    "        step_size (int): Number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch. Default: 2000\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        last_batch_iteration (int): The index of the last batch. Default: -1\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>         train_batch(...)\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "def write_log(logfile, train_loss, test_loss, test_score, lr):\n",
    "    with open(logfile, \"a+\") as log:\n",
    "        log.write(\"{}\\t{}\\t{}\\t{}\\n\".format(train_loss, test_loss, test_score, lr))\n",
    "        \n",
    "        \n",
    "        \n",
    "def aug_train(p=1): \n",
    "    return Compose([Resize(224, 224), \n",
    "                    HorizontalFlip(), \n",
    "                    VerticalFlip(), \n",
    "                    RandomRotate90(), \n",
    "                    Transpose(), \n",
    "                    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n",
    "                    OpticalDistortion(),\n",
    "                    GridDistortion(), \n",
    "                    RandomBrightnessContrast(p=0.3), \n",
    "                    RandomGamma(p=0.3), \n",
    "                    OneOf([HueSaturationValue(hue_shift_limit=20, sat_shift_limit=0.1, val_shift_limit=0.1, p=0.3), \n",
    "                           ChannelShuffle(p=0.3), CLAHE(p=0.3)])], p=p)\n",
    "def aug_val(p=1):\n",
    "    return Compose([\n",
    "        Resize(224, 224)\n",
    "    ], p=p)\n",
    "\n",
    "\n",
    "class DataGenerator(data.Dataset):\n",
    "    \"\"\"Generates dataset for loading.\n",
    "    Args:\n",
    "        ids: images ids\n",
    "        labels: labels of images (1/0)\n",
    "        augment: image augmentation from albumentations\n",
    "        imdir: path tpo folder with images\n",
    "    \"\"\"\n",
    "    def __init__(self, ids, labels, augment, imdir):\n",
    "        'Initialization'\n",
    "        self.ids, self.labels = ids, labels\n",
    "        self.augment = augment\n",
    "        self.imdir = imdir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imid = self.ids[idx]\n",
    "        y = self.labels[idx]\n",
    "        X = self.__load_image(imid)\n",
    "        return X, np.expand_dims(y,0)\n",
    "\n",
    "    def __load_image(self, imid):\n",
    "        imid = imid+'.tif'\n",
    "        im = imread(os.path.join(self.imdir, imid))\n",
    "        if self.augment!=None:\n",
    "            augmented = self.augment(image=im)\n",
    "            im = augmented['image']\n",
    "        im = im/255.0\n",
    "        im = np.rollaxis(im, -1)\n",
    "        return im     \n",
    "    \n",
    "    \n",
    "def make_tta(image):\n",
    "    '''\n",
    "    return 4 pictures  - original, 3*90 rotations, mirror\n",
    "    '''\n",
    "    image_tta = np.zeros((4, image.shape[0], image.shape[1], 3))\n",
    "    image_tta[0] = image\n",
    "    aug = HorizontalFlip(p=1)\n",
    "    image_aug = aug(image=image)['image']\n",
    "    image_tta[1] = image_aug\n",
    "    aug = VerticalFlip(p=1)\n",
    "    image_aug = aug(image=image)['image']\n",
    "    image_tta[2] = image_aug\n",
    "    aug = Transpose(p=1)\n",
    "    image_aug = aug(image=image)['image']\n",
    "    image_tta[3] = image_aug    \n",
    "    image_tta = np.rollaxis(image_tta, -1, 1)\n",
    "    return image_tta\n",
    "def aug_train_heavy(p=1):\n",
    "    return Compose([HorizontalFlip(), VerticalFlip(), RandomRotate90(), Transpose(), RandomBrightnessContrast(p=0.3), RandomGamma(p=0.3), OneOf([HueSaturationValue(hue_shift_limit=20, sat_shift_limit=0.1, val_shift_limit=0.1, p=0.3), ChannelShuffle(p=0.3)])], p=p)\n",
    "heavy_tta = aug_train_heavy()\n",
    "\n",
    "def make_tta_heavy(image, n_images=12):\n",
    "    image_tta = np.zeros((n_images, image.shape[0], image.shape[1], 3))\n",
    "    image_tta[0] = image/255.0\n",
    "    for i in range(1,n_images):\n",
    "        image_aug = heavy_tta(image=image)['image']\n",
    "        image_tta[i] = image_aug/255.0\n",
    "    image_tta = np.rollaxis(image_tta, -1, 1)\n",
    "    return image_tta "
   ],
   "id": "113a666a11f3c071"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
